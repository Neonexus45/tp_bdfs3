# Application PREPROCESSOR

Application de transformation et nettoyage des donn√©es US-Accidents pour la couche Silver du lakehouse.

## üìã Vue d'ensemble

L'application PREPROCESSOR transforme les donn√©es brutes de la couche Bronze vers la couche Silver avec un feature engineering avanc√© et la cr√©ation de tables Hive optimis√©es.

### Fonctionnalit√©s principales

- **Nettoyage des donn√©es** : Gestion des nulls, doublons, outliers pour les 47 colonnes
- **Normalisation** : Standardisation des formats et noms de colonnes
- **Feature Engineering** : Cr√©ation de features temporelles, m√©t√©orologiques, g√©ographiques et d'infrastructure
- **Tables Hive** : Cr√©ation de tables optimis√©es avec bucketing et partitioning
- **Agr√©gations** : Analyses avanc√©es et m√©triques complexes

## üèóÔ∏è Architecture

```
src/applications/preprocessor/
‚îú‚îÄ‚îÄ __init__.py                    # Module principal
‚îú‚îÄ‚îÄ preprocessor_app.py            # Application principale
‚îú‚îÄ‚îÄ data_cleaner.py               # Nettoyage des donn√©es
‚îú‚îÄ‚îÄ feature_engineer.py           # Feature engineering
‚îú‚îÄ‚îÄ hive_table_manager.py         # Gestion tables Hive
‚îú‚îÄ‚îÄ transformation_factory.py     # Factory transformations
‚îú‚îÄ‚îÄ aggregation_engine.py         # Moteur agr√©gations
‚îú‚îÄ‚îÄ test_preprocessor.py          # Tests complets
‚îî‚îÄ‚îÄ README.md                     # Documentation
```

## üîß Composants

### PreprocessorApp
Application principale orchestrant le pipeline de transformation.

**Fonctionnalit√©s :**
- Lecture donn√©es Bronze (HDFS Parquet)
- Orchestration des transformations
- Gestion des erreurs et retry
- M√©triques et logging d√©taill√©

### DataCleaner
Composant de nettoyage et normalisation des donn√©es.

**Transformations :**
- Suppression des doublons bas√©e sur l'ID
- Gestion des valeurs nulles par strat√©gies
- Normalisation des colonnes avec parenth√®ses
- Correction des donn√©es incoh√©rentes
- Suppression des outliers statistiques

**Colonnes normalis√©es :**
```python
{
    'Distance(mi)': 'distance_miles',
    'Temperature(F)': 'temperature_fahrenheit',
    'Wind_Chill(F)': 'wind_chill_fahrenheit',
    'Humidity(%)': 'humidity_percent',
    'Pressure(in)': 'pressure_inches',
    'Visibility(mi)': 'visibility_miles',
    'Wind_Speed(mph)': 'wind_speed_mph',
    'Precipitation(in)': 'precipitation_inches'
}
```

### FeatureEngineer
Composant de feature engineering avanc√©.

**Features temporelles :**
- `accident_hour` (0-23)
- `accident_day_of_week` (1-7)
- `accident_month` (1-12)
- `accident_season` (Spring/Summer/Fall/Winter)
- `is_weekend` (boolean)
- `is_rush_hour` (boolean)

**Features m√©t√©orologiques :**
- `weather_category` (Clear/Rain/Snow/Fog/Other)
- `weather_severity_score` (0-10)
- `visibility_category` (Good/Fair/Poor)
- `temperature_category` (Cold/Mild/Hot)

**Features g√©ographiques :**
- `distance_to_city_center` (miles)
- `urban_rural_classification`
- `state_region` (Northeast/Southeast/Midwest/Southwest/West)
- `population_density_category`

**Features infrastructure :**
- `infrastructure_count` (somme des 13 variables)
- `safety_equipment_score` (pond√©ration des √©quipements)
- `traffic_control_type` (Stop/Signal/None/Multiple)

### HiveTableManager
Gestionnaire des tables Hive optimis√©es.

**Tables cr√©√©es :**

#### accidents_clean
- **Description** : Table principale avec toutes les donn√©es enrichies
- **Format** : ORC avec compression Snappy
- **Bucketing** : 50 buckets par State
- **Partitioning** : Par ann√©e et mois d'accident
- **Colonnes** : 47 originales + features cr√©√©es

#### weather_aggregated
- **Description** : Agr√©gations m√©t√©orologiques par zone/p√©riode
- **Format** : ORC avec compression Snappy
- **Bucketing** : 20 buckets par State
- **Partitioning** : Par ann√©e d'agr√©gation
- **M√©triques** : Statistiques temp√©rature, humidit√©, visibilit√©

#### infrastructure_features
- **Description** : Analyse √©quipements par zone g√©ographique
- **Format** : ORC avec compression Snappy
- **Bucketing** : 20 buckets par State
- **Partitioning** : Par ann√©e d'analyse
- **M√©triques** : Scores de s√©curit√©, taux d'accidents par √©quipement

### TransformationFactory
Factory pour la cr√©ation et gestion des transformations.

**Transformations disponibles :**
- `NullHandlingTransformation` : Gestion des valeurs nulles
- `OutlierRemovalTransformation` : Suppression des outliers
- `DataTypeConversionTransformation` : Conversion des types
- `StringNormalizationTransformation` : Normalisation des cha√Ænes
- `RangeValidationTransformation` : Validation des plages

### AggregationEngine
Moteur d'agr√©gations avanc√©es pour l'analyse.

**Types d'agr√©gations :**
- **Temporelles** : Par heure, jour, mois, saison
- **G√©ographiques** : Par √©tat, r√©gion, zone urbaine/rurale
- **M√©t√©orologiques** : Par condition, temp√©rature, visibilit√©
- **Infrastructure** : Par √©quipement, score de s√©curit√©
- **Analyses avanc√©es** : Percentiles, corr√©lations, rankings

## üöÄ Utilisation

### Ex√©cution de base

```python
from src.applications.preprocessor import PreprocessorApp

# Cr√©ation et ex√©cution
app = PreprocessorApp()
result = app.run()

if result['status'] == 'success':
    print(f"‚úÖ Preprocessing r√©ussi: {result['metrics']['records_written']} enregistrements")
    print(f"üìä Features cr√©√©es: {result['metrics']['features_created']}")
    print(f"üóÉÔ∏è Tables cr√©√©es: {result['metrics']['tables_created']}")
else:
    print(f"‚ùå √âchec: {result['message']}")
```

### Utilisation des composants individuels

```python
from src.applications.preprocessor import DataCleaner, FeatureEngineer

# Nettoyage des donn√©es
cleaner = DataCleaner()
cleaned_df = cleaner.clean_data(raw_df)
cleaning_metrics = cleaner.get_cleaning_metrics()

# Feature engineering
engineer = FeatureEngineer()
enriched_df = engineer.create_features(cleaned_df)
feature_metrics = engineer.get_feature_metrics()
```

### Configuration personnalis√©e

```python
from src.common.config.config_manager import ConfigManager

# Configuration personnalis√©e
config = ConfigManager()
config._config.update({
    'hdfs': {
        'bronze_path': '/custom/bronze/path',
        'silver_path': '/custom/silver/path'
    },
    'hive': {
        'database': 'custom_accidents_db'
    }
})

app = PreprocessorApp(config)
```

## üìä M√©triques et Monitoring

### M√©triques d'ex√©cution
```python
{
    'records_read': 7700000,
    'records_processed': 7650000,
    'records_written': 7650000,
    'features_created': 18,
    'tables_created': 3,
    'cleaning_score': 88.5,
    'transformation_score': 92.3,
    'duration_seconds': 1245.6
}
```

### M√©triques de nettoyage
```python
{
    'records_input': 7700000,
    'records_output': 7650000,
    'duplicates_removed': 25000,
    'nulls_handled': 150000,
    'outliers_removed': 75000,
    'columns_normalized': 8,
    'cleaning_score': 88.5
}
```

### M√©triques de feature engineering
```python
{
    'features_created': 18,
    'temporal_features': 6,
    'weather_features': 4,
    'geographic_features': 4,
    'infrastructure_features': 3,
    'categorical_encodings': 5,
    'feature_engineering_score': 92.3
}
```

## üß™ Tests

### Ex√©cution des tests

```bash
# Tests complets
python src/applications/preprocessor/test_preprocessor.py

# Tests avec couverture
pytest src/applications/preprocessor/test_preprocessor.py --cov=src.applications.preprocessor --cov-report=html

# Tests sp√©cifiques
pytest src/applications/preprocessor/test_preprocessor.py::TestDataCleaner -v
```

### Types de tests
- **Tests unitaires** : Chaque composant individuellement
- **Tests d'int√©gration** : Interaction entre composants
- **Tests de performance** : M√©triques et temps d'ex√©cution
- **Tests de validation** : Qualit√© des donn√©es transform√©es

## ‚öôÔ∏è Configuration

### Variables d'environnement

```bash
# Chemins HDFS
HDFS_BRONZE_PATH=/lakehouse/accidents/bronze
HDFS_SILVER_PATH=/lakehouse/accidents/silver

# Configuration Hive
HIVE_DATABASE=accidents_warehouse
HIVE_METASTORE_URI=thrift://localhost:9083

# Configuration Spark
SPARK_EXECUTOR_MEMORY=4g
SPARK_EXECUTOR_CORES=4
SPARK_SQL_ADAPTIVE_ENABLED=true
```

### Configuration Hive

```yaml
hive:
  database: accidents_warehouse
  warehouse_dir: /user/hive/warehouse
  tables:
    accidents_clean: accidents_clean
    weather_aggregated: weather_aggregated
    infrastructure_features: infrastructure_features
```

## üîç Optimisations

### Optimisations Spark
- **Adaptive Query Execution** : Optimisation automatique des requ√™tes
- **Bucketing** : Distribution optimale des donn√©es
- **Vectorisation ORC** : Lecture vectoris√©e pour de meilleures performances
- **Cache intelligent** : Mise en cache bas√©e sur la taille des donn√©es
- **Coalescing** : √âvitement des petits fichiers

### Optimisations Hive
- **Format ORC** : Compression et indexation optimales
- **Partitioning** : Pruning des partitions pour les requ√™tes
- **Bucketing** : Jointures optimis√©es
- **Statistiques** : Optimisation du plan d'ex√©cution
- **Bloom filters** : Filtrage rapide des donn√©es

## üìà Performance

### Benchmarks typiques
- **Donn√©es d'entr√©e** : 7.7M enregistrements, 47 colonnes
- **Temps de traitement** : ~20 minutes (cluster 4 n≈ìuds)
- **R√©duction de taille** : ~15% apr√®s nettoyage
- **Features cr√©√©es** : 18 nouvelles colonnes
- **Tables Hive** : 3 tables optimis√©es

### Recommandations
- **M√©moire executor** : Minimum 4GB par executor
- **Partitions Spark** : 200-400 partitions pour 7.7M enregistrements
- **Cache** : Activer pour les DataFrames r√©utilis√©s
- **Compression** : Snappy pour √©quilibre performance/taille

## üö® Gestion d'erreurs

### Types d'erreurs g√©r√©es
- **Erreurs Spark** : Session, configuration, ressources
- **Erreurs de donn√©es** : Sch√©ma, qualit√©, corruption
- **Erreurs Hive** : Connexion, permissions, espace disque
- **Erreurs de transformation** : Types, valeurs, logique m√©tier

### Strat√©gies de r√©cup√©ration
- **Retry automatique** : 3 tentatives avec backoff exponentiel
- **Isolation des erreurs** : Continuation malgr√© les erreurs non-critiques
- **Rollback** : Nettoyage en cas d'√©chec complet
- **Logging d√©taill√©** : Tra√ßabilit√© compl√®te des erreurs

## üìö R√©f√©rences

### Documentation technique
- [Architecture Lakehouse](../../docs/architecture_systeme_global.md)
- [Optimisations Spark](../../docs/spark_optimization.md)
- [Patterns de conception](../../docs/design_patterns_classes.md)
- [Logging et monitoring](../../docs/logging_monitoring.md)

### Standards de donn√©es
- [Sch√©ma US-Accidents](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents)
- [Codes d'√©tat am√©ricains](https://en.wikipedia.org/wiki/List_of_U.S._state_and_territory_abbreviations)
- [Conditions m√©t√©orologiques](https://www.weather.gov/bgm/forecast_terms)

---

**Version** : 1.0.0  
**Auteur** : Lakehouse Team  
**Derni√®re mise √† jour** : 2025-06-19