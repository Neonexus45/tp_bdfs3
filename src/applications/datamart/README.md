# DATAMART - Couche Gold Analytics

Application DATAMART pour la crÃ©ation des tables analytiques optimisÃ©es et l'export vers MySQL selon l'architecture Medallion Gold.

## ğŸ¯ Objectif

L'application DATAMART constitue la couche **Gold** de l'architecture Medallion, transformant les donnÃ©es nettoyÃ©es de la couche Silver en tables analytiques optimisÃ©es pour les APIs et les tableaux de bord.

## ğŸ—ï¸ Architecture Medallion - Couche Gold

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BRONZE        â”‚    â”‚    SILVER       â”‚    â”‚     GOLD        â”‚
â”‚   (FEEDER)      â”‚â”€â”€â”€â–¶â”‚ (PREPROCESSOR)  â”‚â”€â”€â”€â–¶â”‚   (DATAMART)    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ DonnÃ©es brutesâ”‚    â”‚ â€¢ Nettoyage     â”‚    â”‚ â€¢ KPIs business â”‚
â”‚ â€¢ HDFS Parquet  â”‚    â”‚ â€¢ Hive ORC      â”‚    â”‚ â€¢ MySQL tables  â”‚
â”‚ â€¢ Partitioning  â”‚    â”‚ â€¢ Features eng. â”‚    â”‚ â€¢ API optimisÃ©  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š FonctionnalitÃ©s Principales

### 1. **AgrÃ©gations Business Multi-dimensionnelles**
- **Temporelles**: Patterns horaires, journaliers, mensuels, annuels
- **GÃ©ographiques**: Analyses par Ã©tat, ville, coordonnÃ©es GPS
- **MÃ©tÃ©orologiques**: CorrÃ©lations conditions mÃ©tÃ©o vs accidents
- **Infrastructure**: EfficacitÃ© des Ã©quipements de sÃ©curitÃ©

### 2. **Calcul des KPIs Business**
- **KPIs SÃ©curitÃ©**: Taux d'accidents par 100k habitants, index dangerositÃ©
- **KPIs Temporels**: Ã‰volution mensuelle/annuelle, pics horaires, saisonnalitÃ©
- **KPIs Infrastructure**: CorrÃ©lation Ã©quipements vs accidents, efficacitÃ©
- **Hotspots**: Top 50 zones les plus dangereuses avec gÃ©olocalisation

### 3. **Export MySQL OptimisÃ©**
- Tables dÃ©normalisÃ©es pour performance API FastAPI
- Indexation composite intelligente pour requÃªtes frÃ©quentes
- Bulk insert optimisÃ© avec gestion des transactions
- Synchronisation incrÃ©mentale des donnÃ©es

### 4. **Optimisation Performance**
- **Spark**: Broadcast joins, cache, coalesce, partitioning
- **MySQL**: Index composites, partitioning par date, query optimization
- **API**: Tables prÃ©-agrÃ©gÃ©es pour rÃ©ponses sub-secondes

## ğŸ—‚ï¸ Structure des Composants

```
src/applications/datamart/
â”œâ”€â”€ __init__.py                 # Package initialization
â”œâ”€â”€ datamart_app.py            # ğŸ¯ Application principale
â”œâ”€â”€ business_aggregator.py     # ğŸ“Š AgrÃ©gations multi-dimensionnelles
â”œâ”€â”€ kpi_calculator.py          # ğŸ“ˆ Calcul des KPIs business
â”œâ”€â”€ mysql_exporter.py          # ğŸ—„ï¸ Export optimisÃ© MySQL
â”œâ”€â”€ table_optimizer.py         # âš¡ Optimisation tables/requÃªtes
â”œâ”€â”€ analytics_engine.py        # ğŸ” Moteur d'analyse avancÃ©e
â”œâ”€â”€ test_datamart.py          # ğŸ§ª Tests complets
â””â”€â”€ README.md                  # ğŸ“š Documentation
```

## ğŸ—„ï¸ Tables MySQL Gold CrÃ©Ã©es

### 1. **accidents_summary** (Table principale dÃ©normalisÃ©e)
```sql
CREATE TABLE accidents_summary (
    id VARCHAR(50) PRIMARY KEY,
    state VARCHAR(2) NOT NULL,
    city VARCHAR(100),
    severity INT,
    accident_date DATE,
    accident_hour INT,
    weather_category VARCHAR(20),
    temperature_category VARCHAR(10),
    infrastructure_count INT,
    safety_score DOUBLE,
    distance_miles DOUBLE,
    INDEX idx_state_date (state, accident_date),
    INDEX idx_severity_weather (severity, weather_category),
    INDEX idx_location (state, city),
    INDEX idx_temporal (accident_date, accident_hour)
) PARTITION BY RANGE (YEAR(accident_date));
```

### 2. **kpis_security** (KPIs sÃ©curitÃ© pour /hotspots)
```sql
CREATE TABLE kpis_security (
    id INT AUTO_INCREMENT PRIMARY KEY,
    state VARCHAR(2) NOT NULL,
    city VARCHAR(100),
    accident_rate_per_100k DOUBLE,
    danger_index DOUBLE,
    severity_distribution JSON,
    hotspot_rank INT,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_state (state),
    INDEX idx_danger_index (danger_index DESC),
    INDEX idx_hotspot_rank (hotspot_rank)
);
```

### 3. **kpis_temporal** (KPIs temporels pour analytics)
```sql
CREATE TABLE kpis_temporal (
    id INT AUTO_INCREMENT PRIMARY KEY,
    period_type ENUM('hourly', 'daily', 'monthly', 'yearly'),
    period_value VARCHAR(20),
    state VARCHAR(2),
    accident_count BIGINT,
    severity_avg DOUBLE,
    trend_direction ENUM('up', 'down', 'stable'),
    seasonal_factor DOUBLE,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_period (period_type, period_value),
    INDEX idx_state_period (state, period_type)
);
```

### 4. **hotspots** (Zones dangereuses pour gÃ©olocalisation)
```sql
CREATE TABLE hotspots (
    id INT AUTO_INCREMENT PRIMARY KEY,
    state VARCHAR(2) NOT NULL,
    city VARCHAR(100),
    latitude DOUBLE,
    longitude DOUBLE,
    accident_count BIGINT,
    severity_avg DOUBLE,
    danger_score DOUBLE,
    radius_miles DOUBLE,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_location (state, city),
    INDEX idx_coordinates (latitude, longitude),
    INDEX idx_danger_score (danger_score DESC)
);
```

## ğŸš€ Utilisation

### ExÃ©cution ComplÃ¨te du Pipeline

```python
from src.applications.datamart.datamart_app import DatamartApp

# Initialisation
app = DatamartApp()

# ExÃ©cution complÃ¨te
result = app.run(force_refresh=False)

print(f"Pipeline terminÃ© en {result['pipeline_duration_seconds']:.2f}s")
print(f"Tables exportÃ©es: {result['export_summary']['tables_created']}")
print(f"Lignes exportÃ©es: {result['export_summary']['total_rows_exported']}")
```

### Mise Ã  Jour IncrÃ©mentale

```python
# Mise Ã  jour depuis un timestamp
result = app.run_incremental_update("2024-01-01T00:00:00")
print(f"Mise Ã  jour incrÃ©mentale: {result['total_rows_exported']} lignes")
```

### ExÃ©cution en Ligne de Commande

```bash
# Pipeline complet
python -m src.applications.datamart.datamart_app

# Avec refresh forcÃ©
python -m src.applications.datamart.datamart_app --force-refresh

# Mise Ã  jour incrÃ©mentale
python -m src.applications.datamart.datamart_app --incremental "2024-01-01T00:00:00"
```

## ğŸ“Š MÃ©triques et KPIs CalculÃ©s

### KPIs de SÃ©curitÃ©
- **Taux d'accidents par 100k habitants** par Ã©tat/ville
- **Index de dangerositÃ©** basÃ© sur sÃ©vÃ©ritÃ© moyenne
- **Distribution de sÃ©vÃ©ritÃ©** par conditions mÃ©tÃ©o
- **Ranking des hotspots** (top 50 zones dangereuses)

### KPIs Temporels
- **Ã‰volution mensuelle/annuelle** des accidents
- **Pics horaires** par jour de semaine
- **Facteurs saisonniers** par type mÃ©tÃ©o
- **DÃ©tection de tendances** (hausse/baisse/stable)

### KPIs Infrastructure
- **EfficacitÃ© des Ã©quipements** (feux, stops, ronds-points)
- **Taux de rÃ©duction d'accidents** par type d'infrastructure
- **Score d'amÃ©lioration sÃ©curitÃ©** par Ã©tat
- **Recommandations automatiques** d'optimisation

### KPIs Performance ML
- **MÃ©triques modÃ¨les** (accuracy, precision, recall, F1-score)
- **Importance des features** par modÃ¨le
- **DÃ©tection de drift** des donnÃ©es
- **Recommandations de re-entraÃ®nement**

## âš¡ Optimisations Spark

### Optimisations de Lecture
```python
# Broadcast joins pour tables de rÃ©fÃ©rence
broadcast_df = broadcast(small_reference_table)
result = large_df.join(broadcast_df, "key")

# Cache des DataFrames rÃ©utilisÃ©s
df.cache()
df.count()  # DÃ©clenche la mise en cache
```

### Optimisations d'Ã‰criture
```python
# Coalesce avant export MySQL
df.coalesce(1).write.mode("overwrite").parquet(path)

# Partitioning pour lecture optimisÃ©e
df.write.partitionBy("state", "year").parquet(path)
```

## ğŸ—„ï¸ Optimisations MySQL

### Index Composites Intelligents
```sql
-- Pour requÃªtes API /hotspots
CREATE INDEX idx_hotspots_api ON hotspots (danger_score DESC, state, city);

-- Pour requÃªtes API /analytics temporelles
CREATE INDEX idx_temporal_api ON kpis_temporal (period_type, state, period_value);

-- Pour requÃªtes API /security rankings
CREATE INDEX idx_security_api ON kpis_security (danger_index DESC, state);
```

### Partitioning par Date
```sql
-- Partitioning de la table principale par annÃ©e
ALTER TABLE accidents_summary 
PARTITION BY RANGE (YEAR(accident_date)) (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

## ğŸ” Analytics AvancÃ©s

### DÃ©tection d'Anomalies
- **Anomalies temporelles**: Pics/creux anormaux d'accidents
- **Anomalies gÃ©ographiques**: Ã‰tats avec taux anormalement Ã©levÃ©s
- **Anomalies mÃ©tÃ©orologiques**: Conditions Ã  impact exceptionnel

### Calcul de CorrÃ©lations
- **MÃ©tÃ©o vs Accidents**: CorrÃ©lation tempÃ©rature/visibilitÃ©/accidents
- **Infrastructure vs SÃ©curitÃ©**: EfficacitÃ© Ã©quipements vs rÃ©duction accidents
- **Patterns temporels**: Facteurs saisonniers vs frÃ©quence accidents

### Insights Business AutomatisÃ©s
- **Top Ã©tats dangereux** avec recommandations prioritaires
- **Heures de pointe** pour renforcement surveillance
- **Infrastructure optimale** Ã  privilÃ©gier par zone
- **CorrÃ©lations significatives** pour stratÃ©gies prÃ©vention

## ğŸ§ª Tests et Validation

### ExÃ©cution des Tests
```bash
# Tests complets
python -m pytest src/applications/datamart/test_datamart.py -v

# Tests avec couverture
python -m pytest src/applications/datamart/test_datamart.py --cov=src.applications.datamart

# Tests d'une classe spÃ©cifique
python -m pytest src/applications/datamart/test_datamart.py::TestDatamartApp -v
```

### Tests de Performance
```python
# Test de charge avec donnÃ©es volumineuses
def test_large_dataset_performance():
    app = DatamartApp()
    start_time = time.time()
    result = app.run()
    duration = time.time() - start_time
    
    assert duration < 300  # Moins de 5 minutes
    assert result['export_summary']['total_rows_exported'] > 10000
```

## ğŸ“ˆ MÃ©triques de Performance

### MÃ©triques Spark
- **DurÃ©e agrÃ©gations business**: < 2 minutes pour 7M+ lignes
- **DurÃ©e calcul KPIs**: < 1 minute pour toutes les dimensions
- **Utilisation mÃ©moire**: OptimisÃ©e avec cache et broadcast joins

### MÃ©triques MySQL
- **DurÃ©e export bulk**: < 30 secondes pour 100k+ lignes
- **Performance requÃªtes API**: < 100ms avec index optimisÃ©s
- **Taille tables Gold**: ~500MB pour dataset complet US-Accidents

### MÃ©triques QualitÃ©
- **ComplÃ©tude donnÃ©es**: > 95% pour toutes les tables Gold
- **CohÃ©rence KPIs**: Validation automatique des calculs
- **FraÃ®cheur donnÃ©es**: Mise Ã  jour incrÃ©mentale < 5 minutes

## ğŸ”§ Configuration

### Variables d'Environnement
```bash
# MySQL Gold
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=accidents_lakehouse
MYSQL_USER=tatane
MYSQL_PASSWORD=tatane
MYSQL_POOL_SIZE=10

# Hive Silver (source)
HIVE_METASTORE_URI=thrift://localhost:9083
HIVE_DATABASE=accidents_warehouse

# Spark Configuration
SPARK_MASTER=yarn
SPARK_EXECUTOR_MEMORY=2g
SPARK_EXECUTOR_CORES=2
SPARK_SQL_SHUFFLE_PARTITIONS=200

# Logging
LOG_LEVEL=INFO
LOG_DATAMART_LEVEL=INFO
```

### Configuration AvancÃ©e
```yaml
# config/datamart.yaml
datamart:
  batch_size: 1000
  max_retries: 3
  retry_delay: 5
  
  kpi_thresholds:
    high_danger_threshold: 2.5
    hotspot_min_accidents: 50
    top_hotspots_count: 50
    
  optimization:
    enable_query_cache: true
    enable_partitioning: true
    index_creation: auto
```

## ğŸš¨ Monitoring et Alertes

### MÃ©triques Ã  Surveiller
- **DurÃ©e pipeline**: Alerte si > 10 minutes
- **Taux d'Ã©chec**: Alerte si > 5%
- **QualitÃ© donnÃ©es**: Alerte si complÃ©tude < 90%
- **Performance MySQL**: Alerte si requÃªtes > 1 seconde

### Logs StructurÃ©s JSON
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "application": "datamart",
  "level": "INFO",
  "message": "Pipeline completed successfully",
  "pipeline_duration_seconds": 180.5,
  "tables_exported": 6,
  "total_rows_exported": 125000,
  "data_quality_score": 96.8
}
```

## ğŸ”„ IntÃ©gration avec l'Ã‰cosystÃ¨me

### DÃ©pendances Amont (Silver)
- **PREPROCESSOR**: Tables Hive nettoyÃ©es et enrichies
- **DonnÃ©es requises**: accidents_clean, weather_aggregated, infrastructure_features

### Consommateurs Aval (API)
- **FastAPI**: Endpoints optimisÃ©s avec tables Gold MySQL
- **Tableaux de bord**: Visualisations temps rÃ©el des KPIs
- **Alertes**: Notifications basÃ©es sur seuils KPIs

### Flux de DonnÃ©es
```
Hive Silver â†’ DATAMART â†’ MySQL Gold â†’ FastAPI â†’ Frontend
     â†“              â†“           â†“          â†“
  ORC Tables â†’ Spark Jobs â†’ Index Tables â†’ Sub-second â†’ Real-time
                                         Queries      Dashboards
```

## ğŸ¯ Roadmap et AmÃ©liorations

### Phase 1 (Actuelle)
- âœ… Pipeline batch complet
- âœ… KPIs business essentiels
- âœ… Export MySQL optimisÃ©
- âœ… Tests complets

### Phase 2 (Prochaine)
- ğŸ”„ Streaming temps rÃ©el avec Kafka
- ğŸ”„ ML automatisÃ© pour prÃ©dictions
- ğŸ”„ Alertes intelligentes
- ğŸ”„ Dashboard interactif

### Phase 3 (Future)
- ğŸ“‹ Multi-tenancy par rÃ©gion
- ğŸ“‹ API GraphQL avancÃ©e
- ğŸ“‹ Cache distribuÃ© Redis
- ğŸ“‹ DÃ©ploiement Kubernetes

---

## ğŸ“ Support

Pour toute question ou problÃ¨me:
- **Documentation**: Voir `/docs` pour architecture dÃ©taillÃ©e
- **Tests**: ExÃ©cuter `pytest` pour validation
- **Logs**: Consulter `/var/log/lakehouse-accidents/datamart.log`
- **Monitoring**: Dashboard Grafana disponible sur port 3000

**DATAMART - Transforming Silver Data into Gold Analytics** âœ¨