# Structure des RÃ©pertoires et Organisation des Fichiers

## ðŸ“ Arborescence ComplÃ¨te du Projet

```
lakehouse_accidents_us/
â”œâ”€â”€ .env                              # Configuration environnement
â”œâ”€â”€ .gitignore                        # Fichiers Ã  ignorer par Git
â”œâ”€â”€ requirements.txt                  # DÃ©pendances Python
â”œâ”€â”€ setup.py                         # Installation package
â”œâ”€â”€ README.md                        # Documentation projet
â”œâ”€â”€ Dockerfile                       # Conteneurisation (optionnel)
â”œâ”€â”€ docker-compose.yml               # Orchestration services
â”‚
â”œâ”€â”€ config/                          # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_manager.py            # Gestion configuration centralisÃ©e
â”‚   â”œâ”€â”€ spark_config.py              # Configuration Spark optimisÃ©e
â”‚   â”œâ”€â”€ database_schemas.py          # SchÃ©mas MySQL et Hive
â”‚   â””â”€â”€ logging_config.yaml          # Configuration logging structurÃ©
â”‚
â”œâ”€â”€ src/                             # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ common/                      # Composants rÃ©utilisables
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_spark_app.py        # Classe abstraite applications Spark
â”‚   â”‚   â”œâ”€â”€ structured_logger.py     # Logging structurÃ©
â”‚   â”‚   â”œâ”€â”€ data_validator.py        # Validation donnÃ©es et schÃ©mas
â”‚   â”‚   â”œâ”€â”€ spark_optimizer.py       # Optimisations Spark
â”‚   â”‚   â”œâ”€â”€ database_manager.py      # Gestion connexions MySQL
â”‚   â”‚   â””â”€â”€ exceptions.py            # Exceptions personnalisÃ©es
â”‚   â”‚
â”‚   â”œâ”€â”€ applications/                # 4 Applications Spark principales
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ feeder/                  # Application d'ingestion
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ feeder_app.py        # Application principale ingestion
â”‚   â”‚   â”‚   â”œâ”€â”€ csv_ingester.py      # Ingestion CSV batch
â”‚   â”‚   â”‚   â”œâ”€â”€ stream_ingester.py   # Ingestion streaming
â”‚   â”‚   â”‚   â”œâ”€â”€ data_generator.py    # GÃ©nÃ©rateur donnÃ©es temps rÃ©el
â”‚   â”‚   â”‚   â”œâ”€â”€ schema_validator.py  # Validation schÃ©ma 47 colonnes
â”‚   â”‚   â”‚   â””â”€â”€ partitioner.py       # Partitioning date/Ã©tat
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ preprocessor/            # Application de preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessor_app.py  # Application principale preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ data_cleaner.py      # Nettoyage donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_engineer.py  # Feature engineering
â”‚   â”‚   â”‚   â”œâ”€â”€ weather_aggregator.py # AgrÃ©gations mÃ©tÃ©orologiques
â”‚   â”‚   â”‚   â”œâ”€â”€ infrastructure_analyzer.py # Analyse infrastructure
â”‚   â”‚   â”‚   â”œâ”€â”€ temporal_analyzer.py  # Patterns temporels
â”‚   â”‚   â”‚   â”œâ”€â”€ geographic_analyzer.py # Hotspots gÃ©ographiques
â”‚   â”‚   â”‚   â””â”€â”€ hive_table_creator.py # CrÃ©ation tables Hive
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ datamart/               # Application datamart
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ datamart_app.py      # Application principale datamart
â”‚   â”‚   â”‚   â”œâ”€â”€ kpi_calculator.py    # Calcul KPIs mÃ©tier
â”‚   â”‚   â”‚   â”œâ”€â”€ hotspots_generator.py # GÃ©nÃ©ration zones Ã  risque
â”‚   â”‚   â”‚   â”œâ”€â”€ temporal_reporter.py  # Rapports patterns temporels
â”‚   â”‚   â”‚   â”œâ”€â”€ mysql_exporter.py    # Export vers MySQL Gold
â”‚   â”‚   â”‚   â””â”€â”€ aggregation_engine.py # Moteur d'agrÃ©gation
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ml_training/             # Application ML
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ ml_training_app.py   # Application principale ML
â”‚   â”‚       â”œâ”€â”€ feature_selector.py  # SÃ©lection features
â”‚   â”‚       â”œâ”€â”€ model_trainer.py     # EntraÃ®nement modÃ¨les
â”‚   â”‚       â”œâ”€â”€ model_evaluator.py   # Ã‰valuation performance
â”‚   â”‚       â”œâ”€â”€ prediction_service.py # Service prÃ©dictions
â”‚   â”‚       â”œâ”€â”€ mlflow_integration.py # IntÃ©gration MLflow
â”‚   â”‚       â””â”€â”€ model_registry.py    # Registre des modÃ¨les
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                         # API FastAPI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                  # Point d'entrÃ©e FastAPI
â”‚   â”‚   â”œâ”€â”€ dependencies.py          # DÃ©pendances communes
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ routers/                 # Routeurs par domaine mÃ©tier
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ accidents.py         # Endpoints donnÃ©es accidents
â”‚   â”‚   â”‚   â”œâ”€â”€ hotspots.py          # Endpoints zones Ã  risque
â”‚   â”‚   â”‚   â”œâ”€â”€ kpis.py              # Endpoints KPIs mÃ©tier
â”‚   â”‚   â”‚   â”œâ”€â”€ predictions.py       # Endpoints prÃ©dictions ML
â”‚   â”‚   â”‚   â””â”€â”€ health.py            # Endpoints santÃ© systÃ¨me
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/                  # ModÃ¨les Pydantic
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ request_models.py    # ModÃ¨les requÃªtes
â”‚   â”‚   â”‚   â”œâ”€â”€ response_models.py   # ModÃ¨les rÃ©ponses
â”‚   â”‚   â”‚   â””â”€â”€ base_models.py       # ModÃ¨les de base
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/                # Services mÃ©tier API
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ accident_service.py  # Service accidents
â”‚   â”‚   â”‚   â”œâ”€â”€ hotspot_service.py   # Service hotspots
â”‚   â”‚   â”‚   â”œâ”€â”€ kpi_service.py       # Service KPIs
â”‚   â”‚   â”‚   â””â”€â”€ prediction_service.py # Service prÃ©dictions
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ middleware/              # Middlewares
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ rate_limiting.py    # Limitation de dÃ©bit
â”‚   â”‚       â””â”€â”€ cors.py             # Configuration CORS
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Utilitaires transversaux
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ date_utils.py            # Utilitaires dates et temps
â”‚       â”œâ”€â”€ geo_utils.py             # Utilitaires gÃ©ographiques
â”‚       â”œâ”€â”€ spark_utils.py           # Utilitaires Spark
â”‚       â”œâ”€â”€ validation_utils.py      # Utilitaires validation
â”‚       â”œâ”€â”€ file_utils.py            # Utilitaires fichiers
â”‚       â””â”€â”€ constants.py             # Constantes projet
â”‚
â”œâ”€â”€ data/                            # DonnÃ©es du projet
â”‚   â”œâ”€â”€ raw/                         # DonnÃ©es brutes
â”‚   â”‚   â””â”€â”€ US_Accidents_March23.csv # Dataset principal
â”‚   â”œâ”€â”€ bronze/                      # Couche Bronze (sera crÃ©Ã©e)
â”‚   â”œâ”€â”€ silver/                      # Couche Silver (sera crÃ©Ã©e)
â”‚   â”œâ”€â”€ gold/                        # Couche Gold (sera crÃ©Ã©e)
â”‚   â””â”€â”€ models/                      # ModÃ¨les ML sauvegardÃ©s
â”‚
â”œâ”€â”€ scripts/                         # Scripts de dÃ©ploiement et maintenance
â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â”œâ”€â”€ deploy_applications.sh   # DÃ©ploiement applications Spark
â”‚   â”‚   â”œâ”€â”€ setup_environment.sh     # Configuration environnement
â”‚   â”‚   â””â”€â”€ start_services.sh        # DÃ©marrage services
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ create_mysql_tables.sql  # CrÃ©ation tables MySQL
â”‚   â”‚   â”œâ”€â”€ create_hive_tables.hql   # CrÃ©ation tables Hive
â”‚   â”‚   â””â”€â”€ setup_indexes.sql        # CrÃ©ation index optimisÃ©s
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ health_check.py          # VÃ©rification santÃ© systÃ¨me
â”‚   â”‚   â””â”€â”€ performance_monitor.py   # Monitoring performance
â”‚   â””â”€â”€ data_generation/
â”‚       â”œâ”€â”€ generate_sample_data.py  # GÃ©nÃ©ration donnÃ©es test
â”‚       â””â”€â”€ simulate_streaming.py    # Simulation streaming
â”‚
â”œâ”€â”€ tests/                           # Tests automatisÃ©s
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                  # Configuration pytest
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/                        # Tests unitaires
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_data_validator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_spark_optimizer.py
â”‚   â”‚   â”‚   â””â”€â”€ test_structured_logger.py
â”‚   â”‚   â”œâ”€â”€ applications/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_feeder_app.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_preprocessor_app.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_datamart_app.py
â”‚   â”‚   â”‚   â””â”€â”€ test_ml_training_app.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_accident_endpoints.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_hotspot_endpoints.py
â”‚   â”‚   â”‚   â””â”€â”€ test_prediction_endpoints.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ test_date_utils.py
â”‚   â”‚       â””â”€â”€ test_geo_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/                 # Tests d'intÃ©gration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_end_to_end_pipeline.py
â”‚   â”‚   â”œâ”€â”€ test_api_database_integration.py
â”‚   â”‚   â””â”€â”€ test_spark_hive_integration.py
â”‚   â”‚
â”‚   â”œâ”€â”€ performance/                 # Tests de performance
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_spark_performance.py
â”‚   â”‚   â””â”€â”€ test_api_performance.py
â”‚   â”‚
â”‚   â””â”€â”€ fixtures/                    # DonnÃ©es de test
â”‚       â”œâ”€â”€ sample_accidents.csv
â”‚       â”œâ”€â”€ test_config.env
â”‚       â””â”€â”€ mock_responses.json
â”‚
â”œâ”€â”€ docs/                            # Documentation technique
â”‚   â”œâ”€â”€ architecture_systeme_global.md
â”‚   â”œâ”€â”€ design_patterns_classes.md
â”‚   â”œâ”€â”€ structure_repertoires.md
â”‚   â”œâ”€â”€ configuration_management.md
â”‚   â”œâ”€â”€ database_schema.md
â”‚   â”œâ”€â”€ spark_optimization.md
â”‚   â”œâ”€â”€ api_design.md
â”‚   â”œâ”€â”€ logging_monitoring.md
â”‚   â”œâ”€â”€ deployment_testing.md
â”‚   â”œâ”€â”€ user_guide.md
â”‚   â””â”€â”€ troubleshooting.md
â”‚
â”œâ”€â”€ monitoring/                      # Monitoring et observabilitÃ©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ spark_metrics.py         # MÃ©triques Spark
â”‚   â”‚   â”œâ”€â”€ api_metrics.py           # MÃ©triques API
â”‚   â”‚   â””â”€â”€ business_metrics.py      # MÃ©triques mÃ©tier
â”‚   â”œâ”€â”€ alerts/
â”‚   â”‚   â”œâ”€â”€ alert_manager.py         # Gestionnaire d'alertes
â”‚   â”‚   â””â”€â”€ notification_service.py  # Service notifications
â”‚   â””â”€â”€ dashboards/
â”‚       â”œâ”€â”€ grafana_config.json      # Configuration Grafana
â”‚       â””â”€â”€ prometheus_config.yml    # Configuration Prometheus
â”‚
â””â”€â”€ notebooks/                       # Notebooks d'analyse (optionnel)
    â”œâ”€â”€ data_exploration.ipynb       # Exploration donnÃ©es
    â”œâ”€â”€ model_analysis.ipynb         # Analyse modÃ¨les ML
    â””â”€â”€ performance_analysis.ipynb   # Analyse performance
```

## ðŸŽ¯ Principes d'Organisation

### SÃ©paration des ResponsabilitÃ©s
- **`src/common/`** : Composants rÃ©utilisables par toutes les applications
- **`src/applications/`** : Applications Spark spÃ©cialisÃ©es par domaine
- **`src/api/`** : Interface REST avec sÃ©paration routers/services/models
- **`src/utils/`** : Utilitaires transversaux sans dÃ©pendances mÃ©tier

### Convention de Nommage
- **Fichiers** : snake_case (ex: `data_validator.py`)
- **Classes** : PascalCase (ex: `DataValidator`)
- **MÃ©thodes/Variables** : snake_case (ex: `validate_schema`)
- **Constantes** : UPPER_SNAKE_CASE (ex: `MAX_BATCH_SIZE`)

### Structure Modulaire
- Chaque module a sa responsabilitÃ© unique
- DÃ©pendances claires et minimales
- FacilitÃ© de test et maintenance
- ExtensibilitÃ© pour nouvelles fonctionnalitÃ©s

## ðŸ“¦ Gestion des DÃ©pendances

### requirements.txt
```txt
# Spark et Big Data
pyspark==3.4.0
py4j==0.10.9.7

# Base de donnÃ©es
pymysql==1.0.3
sqlalchemy==2.0.15
hive-thrift-py==0.7.0

# API et Web
fastapi==0.100.0
uvicorn==0.22.0
pydantic==2.0.0

# Machine Learning
scikit-learn==1.3.0
mlflow==2.4.0
numpy==1.24.0
pandas==2.0.0

# Utilitaires
python-dotenv==1.0.0
pyyaml==6.0
structlog==23.1.0
click==8.1.0

# Tests
pytest==7.4.0
pytest-cov==4.1.0
pytest-mock==3.11.0

# DÃ©veloppement
black==23.3.0
flake8==6.0.0
mypy==1.4.0
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="lakehouse-accidents-us",
    version="1.0.0",
    description="Architecture Lakehouse pour l'analyse des accidents US",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.8",
    install_requires=[
        "pyspark>=3.4.0",
        "fastapi>=0.100.0",
        "pymysql>=1.0.3",
        # ... autres dÃ©pendances
    ],
    entry_points={
        "console_scripts": [
            "feeder=applications.feeder.feeder_app:main",
            "preprocessor=applications.preprocessor.preprocessor_app:main",
            "datamart=applications.datamart.datamart_app:main",
            "ml-training=applications.ml_training.ml_training_app:main",
        ],
    },
)
```

## ðŸ”§ Configuration par Environnement

### Structure de Configuration
```
config/
â”œâ”€â”€ base.yaml              # Configuration de base
â”œâ”€â”€ development.yaml       # Surcharges dÃ©veloppement
â”œâ”€â”€ staging.yaml          # Surcharges staging
â”œâ”€â”€ production.yaml       # Surcharges production
â””â”€â”€ secrets/              # Secrets par environnement
    â”œâ”€â”€ dev_secrets.env
    â”œâ”€â”€ staging_secrets.env
    â””â”€â”€ prod_secrets.env
```

## ðŸ“‹ Standards de DÃ©veloppement

### Formatage du Code
- **Black** : Formatage automatique Python
- **Flake8** : VÃ©rification style et qualitÃ©
- **MyPy** : VÃ©rification types statiques

### Documentation
- **Docstrings** : Format Google style
- **Type Hints** : Obligatoires pour toutes les fonctions publiques
- **README** : Documentation utilisateur et dÃ©veloppeur

### Tests
- **Coverage** : Minimum 80% de couverture
- **Tests unitaires** : Un fichier de test par module
- **Tests d'intÃ©gration** : Validation des flux complets
- **Tests de performance** : Validation des optimisations Spark

## ðŸš€ DÃ©ploiement et Maintenance

### Scripts de DÃ©ploiement
- **Automatisation** : Scripts bash pour dÃ©ploiement
- **Validation** : VÃ©rification santÃ© aprÃ¨s dÃ©ploiement
- **Rollback** : ProcÃ©dure de retour arriÃ¨re

### Monitoring
- **Logs structurÃ©s** : Format JSON pour parsing automatique
- **MÃ©triques** : Collecte via Prometheus/Grafana
- **Alertes** : Notification automatique des problÃ¨mes

Cette structure garantit une organisation claire, une maintenance facilitÃ©e et une Ã©volutivitÃ© optimale pour le projet lakehouse d'analyse des accidents US.