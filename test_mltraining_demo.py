#!/usr/bin/env python3
"""
Script de d√©monstration pour l'application MLTRAINING

Ce script teste l'application MLTRAINING compl√®te avec des donn√©es simul√©es
pour d√©montrer toutes les fonctionnalit√©s de classification de s√©v√©rit√©.
"""

import os
import sys
import time
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
import pandas as pd

# Ajout du chemin pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.common.config.config_manager import ConfigManager
from src.common.utils.logger import Logger
from src.applications.mltraining.mltraining_app import MLTrainingApp
from src.applications.mltraining.feature_processor import FeatureProcessor
from src.applications.mltraining.model_trainer import ModelTrainer
from src.applications.mltraining.model_evaluator import ModelEvaluator
from src.applications.mltraining.prediction_service import PredictionService


def create_demo_data(spark: SparkSession, num_records: int = 1000):
    """Cr√©e des donn√©es de d√©monstration pour l'entra√Ænement ML"""
    print(f"üîß Cr√©ation de {num_records} enregistrements de d√©monstration...")
    
    # Sch√©ma des donn√©es
    schema = StructType([
        StructField("Severity", IntegerType(), True),
        StructField("Distance_mi", DoubleType(), True),
        StructField("Temperature_F", DoubleType(), True),
        StructField("Wind_Chill_F", DoubleType(), True),
        StructField("Humidity_percent", DoubleType(), True),
        StructField("Pressure_in", DoubleType(), True),
        StructField("Visibility_mi", DoubleType(), True),
        StructField("Wind_Speed_mph", DoubleType(), True),
        StructField("Precipitation_in", DoubleType(), True),
        StructField("Start_Lat", DoubleType(), True),
        StructField("Start_Lng", DoubleType(), True),
        StructField("State", StringType(), True),
        StructField("Weather_Condition", StringType(), True),
        StructField("Source", StringType(), True),
        StructField("Sunrise_Sunset", StringType(), True),
        StructField("Civil_Twilight", StringType(), True),
        StructField("Nautical_Twilight", StringType(), True),
        StructField("Astronomical_Twilight", StringType(), True),
        StructField("Amenity", StringType(), True),
        StructField("Bump", StringType(), True),
        StructField("Crossing", StringType(), True),
        StructField("Give_Way", StringType(), True),
        StructField("Junction", StringType(), True),
        StructField("No_Exit", StringType(), True),
        StructField("Railway", StringType(), True),
        StructField("Roundabout", StringType(), True),
        StructField("Station", StringType(), True),
        StructField("Stop", StringType(), True),
        StructField("Traffic_Calming", StringType(), True),
        StructField("Traffic_Signal", StringType(), True),
        StructField("Turning_Loop", StringType(), True),
        StructField("accident_hour", IntegerType(), True),
        StructField("accident_day_of_week", IntegerType(), True),
        StructField("accident_month", IntegerType(), True),
        StructField("accident_season", StringType(), True),
        StructField("is_weekend", StringType(), True),
        StructField("is_rush_hour", StringType(), True),
        StructField("weather_category", StringType(), True),
        StructField("weather_severity_score", DoubleType(), True),
        StructField("visibility_category", StringType(), True),
        StructField("temperature_category", StringType(), True),
        StructField("distance_to_city_center", DoubleType(), True),
        StructField("urban_rural_classification", StringType(), True),
        StructField("state_region", StringType(), True),
        StructField("population_density_category", StringType(), True),
        StructField("infrastructure_count", IntegerType(), True),
        StructField("safety_equipment_score", DoubleType(), True),
        StructField("traffic_control_type", StringType(), True)
    ])
    
    # G√©n√©ration de donn√©es r√©alistes
    import random
    random.seed(42)
    
    data = []
    states = ["CA", "NY", "TX", "FL", "IL", "PA", "OH", "GA", "NC", "MI"]
    weather_conditions = ["Clear", "Rain", "Snow", "Cloudy", "Fog", "Windy"]
    sources = ["Source1", "Source2", "Source3"]
    regions = ["West", "Northeast", "South", "Midwest"]
    
    for i in range(num_records):
        # S√©v√©rit√© avec distribution r√©aliste (plus de cas mineurs)
        severity_weights = [0.4, 0.35, 0.2, 0.05]  # 1, 2, 3, 4
        severity = random.choices([1, 2, 3, 4], weights=severity_weights)[0]
        
        # Variables corr√©l√©es avec la s√©v√©rit√©
        base_distance = 0.1 + (severity - 1) * 0.5 + random.uniform(-0.2, 0.2)
        base_visibility = 15.0 - (severity - 1) * 3.0 + random.uniform(-2, 2)
        base_weather_score = severity * 0.5 + random.uniform(0, 1)
        
        # Coordonn√©es g√©ographiques r√©alistes (√âtats-Unis)
        lat = 25.0 + random.uniform(0, 25)  # 25-50¬∞N
        lng = -125.0 + random.uniform(0, 50)  # -125 √† -75¬∞W
        
        record = (
            severity,
            max(0.1, base_distance),  # Distance
            random.uniform(20, 100),  # Temperature
            random.uniform(15, 95),   # Wind Chill
            random.uniform(20, 100),  # Humidity
            random.uniform(28, 32),   # Pressure
            max(0.1, base_visibility), # Visibility
            random.uniform(0, 30),    # Wind Speed
            random.uniform(0, 2),     # Precipitation
            lat, lng,                 # Coordinates
            random.choice(states),    # State
            random.choice(weather_conditions),  # Weather
            random.choice(sources),   # Source
            random.choice(["Day", "Night"]),    # Sunrise_Sunset
            random.choice(["Day", "Night"]),    # Civil_Twilight
            random.choice(["Day", "Night"]),    # Nautical_Twilight
            random.choice(["Day", "Night"]),    # Astronomical_Twilight
            # Infrastructure bool√©enne (13 colonnes)
            *[random.choice(["True", "False"]) for _ in range(13)],
            # Features temporelles
            random.randint(0, 23),    # hour
            random.randint(1, 7),     # day_of_week
            random.randint(1, 12),    # month
            random.choice(["Spring", "Summer", "Fall", "Winter"]),  # season
            random.choice(["True", "False"]),  # is_weekend
            random.choice(["True", "False"]),  # is_rush_hour
            # Features m√©t√©orologiques
            random.choice(["Clear", "Adverse", "Moderate"]),  # weather_category
            base_weather_score,       # weather_severity_score
            random.choice(["Good", "Poor", "Moderate"]),      # visibility_category
            random.choice(["Cold", "Mild", "Hot"]),           # temperature_category
            # Features g√©ographiques
            random.uniform(0, 50),    # distance_to_city_center
            random.choice(["Urban", "Rural", "Suburban"]),    # urban_rural
            random.choice(regions),   # state_region
            random.choice(["Low", "Medium", "High"]),         # population_density
            # Features infrastructure
            random.randint(0, 5),     # infrastructure_count
            random.uniform(1, 5),     # safety_equipment_score
            random.choice(["Signal", "Stop", "None"])         # traffic_control_type
        )
        
        data.append(record)
    
    df = spark.createDataFrame(data, schema)
    print(f"‚úÖ Donn√©es cr√©√©es: {df.count()} enregistrements, {len(df.columns)} colonnes")
    
    # Affichage de la distribution des s√©v√©rit√©s
    severity_dist = df.groupBy("Severity").count().orderBy("Severity").collect()
    print("üìä Distribution des s√©v√©rit√©s:")
    for row in severity_dist:
        print(f"   S√©v√©rit√© {row['Severity']}: {row['count']} accidents")
    
    return df


def demo_feature_processing(spark: SparkSession, data: any):
    """D√©monstration du traitement des features"""
    print("\n" + "="*60)
    print("üîß D√âMONSTRATION - TRAITEMENT DES FEATURES")
    print("="*60)
    
    config = ConfigManager()
    feature_processor = FeatureProcessor(config, spark)
    
    print("üìã Analyse initiale des donn√©es...")
    start_time = time.time()
    
    # Pr√©paration des features
    processed_df, feature_info = feature_processor.prepare_features_for_ml(data)
    
    processing_time = time.time() - start_time
    
    print(f"‚úÖ Traitement termin√© en {processing_time:.2f} secondes")
    print(f"üìä Features s√©lectionn√©es: {len(feature_info['selected_features'])}")
    print(f"üéØ Colonne cible: {feature_info['target_column']}")
    print(f"üìà M√©triques de traitement:")
    
    for metric, value in feature_info['processing_metrics'].items():
        print(f"   {metric}: {value}")
    
    return processed_df, feature_info


def demo_model_training(spark: SparkSession, processed_df: any, feature_info: dict):
    """D√©monstration de l'entra√Ænement des mod√®les"""
    print("\n" + "="*60)
    print("ü§ñ D√âMONSTRATION - ENTRA√éNEMENT DES MOD√àLES")
    print("="*60)
    
    config = ConfigManager()
    trainer = ModelTrainer(config, spark)
    
    # Division des donn√©es
    train_df, temp_df = processed_df.randomSplit([0.8, 0.2], seed=42)
    val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)
    
    print(f"üìä Division des donn√©es:")
    print(f"   Entra√Ænement: {train_df.count()} √©chantillons")
    print(f"   Validation: {val_df.count()} √©chantillons")
    print(f"   Test: {test_df.count()} √©chantillons")
    
    print("\nüöÄ Entra√Ænement des mod√®les...")
    start_time = time.time()
    
    # Entra√Ænement de mod√®les individuels pour la d√©mo
    models_results = {}
    
    # Random Forest
    print("   üå≤ Entra√Ænement Random Forest...")
    rf_result = trainer.train_single_model(
        'random_forest', train_df, val_df, 
        feature_info['selected_features'], 'Severity'
    )
    models_results['random_forest'] = rf_result
    
    # Logistic Regression (plus rapide pour la d√©mo)
    print("   üìà Entra√Ænement Logistic Regression...")
    lr_result = trainer.train_single_model(
        'logistic_regression', train_df, val_df,
        feature_info['selected_features'], 'Severity'
    )
    models_results['logistic_regression'] = lr_result
    
    training_time = time.time() - start_time
    
    print(f"\n‚úÖ Entra√Ænement termin√© en {training_time:.2f} secondes")
    print("üìä R√©sultats des mod√®les:")
    
    for model_name, result in models_results.items():
        metrics = result['validation_metrics']
        print(f"\n   {model_name.upper()}:")
        print(f"     Accuracy: {metrics['accuracy']:.4f}")
        print(f"     F1-Score: {metrics['f1_score_macro']:.4f}")
        print(f"     Precision: {metrics['precision_macro']:.4f}")
        print(f"     Recall: {metrics['recall_macro']:.4f}")
        print(f"     Temps d'entra√Ænement: {result['training_time_seconds']:.2f}s")
    
    return models_results, test_df


def demo_model_evaluation(spark: SparkSession, models_results: dict, test_df: any, feature_info: dict):
    """D√©monstration de l'√©valuation des mod√®les"""
    print("\n" + "="*60)
    print("üìä D√âMONSTRATION - √âVALUATION DES MOD√àLES")
    print("="*60)
    
    config = ConfigManager()
    evaluator = ModelEvaluator(config, spark)
    
    evaluation_results = []
    
    for model_name, model_result in models_results.items():
        print(f"\nüîç √âvaluation du mod√®le {model_name.upper()}...")
        
        evaluation = evaluator.evaluate_model(
            model=model_result['model'],
            test_df=test_df,
            feature_columns=feature_info['selected_features'],
            target_column='Severity',
            model_name=model_name
        )
        
        evaluation_results.append(evaluation)
        
        print(f"üìà M√©triques globales:")
        global_metrics = evaluation['global_metrics']
        for metric, value in global_metrics.items():
            print(f"   {metric}: {value:.4f}")
        
        print(f"üìã M√©triques par classe:")
        class_metrics = evaluation['class_metrics']
        for class_name, metrics in class_metrics.items():
            print(f"   {class_name}: F1={metrics['f1_score']:.3f}, "
                  f"Precision={metrics['precision']:.3f}, "
                  f"Recall={metrics['recall']:.3f}, "
                  f"Support={metrics['support']}")
        
        print(f"üéØ Matrice de confusion:")
        confusion = evaluation['confusion_matrix']
        print(f"   Total √©chantillons: {confusion['total_samples']}")
        
        if evaluation['feature_importance']:
            print(f"üîù Top 5 features importantes:")
            top_features = list(evaluation['feature_importance'].items())[:5]
            for feature, importance in top_features:
                print(f"   {feature}: {importance:.4f}")
    
    # Comparaison des mod√®les
    if len(evaluation_results) > 1:
        print(f"\nüèÜ COMPARAISON DES MOD√àLES:")
        comparison = evaluator.compare_models(evaluation_results)
        
        if comparison.get('ranking_by_f1_macro'):
            print("üìä Classement par F1-Score:")
            for i, model_data in enumerate(comparison['ranking_by_f1_macro'], 1):
                print(f"   {i}. {model_data['model_name']}: {model_data['f1_score_macro']:.4f}")
        
        best_model = comparison.get('overall_best_model')
        if best_model:
            print(f"ü•á Meilleur mod√®le: {best_model}")
    
    return evaluation_results


def demo_prediction_service(spark: SparkSession, models_results: dict, feature_info: dict):
    """D√©monstration du service de pr√©diction"""
    print("\n" + "="*60)
    print("üîÆ D√âMONSTRATION - SERVICE DE PR√âDICTION")
    print("="*60)
    
    config = ConfigManager()
    predictor = PredictionService(config, spark)
    
    # Simulation du chargement d'un mod√®le (on utilise le premier mod√®le disponible)
    if models_results:
        model_name = list(models_results.keys())[0]
        model = models_results[model_name]['model']
        
        print(f"üì• Simulation du chargement du mod√®le: {model_name}")
        
        # Simulation d'une pr√©diction unitaire
        print("\nüéØ Pr√©diction unitaire:")
        sample_accident = {
            'Distance_mi': 0.8,
            'Temperature_F': 45.0,
            'Humidity_percent': 85.0,
            'Pressure_in': 29.5,
            'Visibility_mi': 3.0,
            'Start_Lat': 40.7128,
            'Start_Lng': -74.0060
        }
        
        print("üìã Donn√©es d'entr√©e:")
        for key, value in sample_accident.items():
            print(f"   {key}: {value}")
        
        # Conversion en DataFrame pour la pr√©diction
        input_df = predictor._dict_to_dataframe(sample_accident)
        
        # Preprocessing et pr√©diction simul√©e
        try:
            processed_input = predictor._preprocess_for_prediction(input_df)
            predictions = model.transform(processed_input)
            
            # Extraction du r√©sultat
            if predictions.count() > 0:
                result_row = predictions.collect()[0]
                predicted_severity = int(result_row['prediction'])
                severity_names = {1: 'Minor', 2: 'Moderate', 3: 'Serious', 4: 'Severe'}
                
                print(f"\n‚úÖ R√©sultat de la pr√©diction:")
                print(f"   S√©v√©rit√© pr√©dite: {predicted_severity} ({severity_names[predicted_severity]})")
                print(f"   Confiance: √âlev√©e")
            else:
                print("‚ùå Erreur lors de la pr√©diction")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Simulation de pr√©diction (erreur attendue): {str(e)[:100]}...")
            print("   (Normal en mode d√©mo sans mod√®le MLflow complet)")
    
    # Informations sur le service
    print(f"\nüìä Informations du service:")
    model_info = predictor.get_model_info()
    for key, value in model_info.items():
        if key != 'feature_columns':  # √âviter l'affichage de la longue liste
            print(f"   {key}: {value}")
    
    metrics = predictor.get_service_metrics()
    print(f"üìà M√©triques du service:")
    for key, value in metrics.items():
        print(f"   {key}: {value}")


def main():
    """Fonction principale de d√©monstration"""
    print("üöÄ D√âMONSTRATION COMPL√àTE - APPLICATION MLTRAINING")
    print("=" * 80)
    print("Classification de s√©v√©rit√© des accidents de la route")
    print("Architecture Medallion - Couche ML")
    print("=" * 80)
    
    # Configuration Spark pour la d√©mo
    spark = SparkSession.builder \
        .appName("MLTraining-Demo") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")  # R√©duire les logs Spark
    
    try:
        start_time = time.time()
        
        # 1. Cr√©ation des donn√©es de d√©monstration
        demo_data = create_demo_data(spark, num_records=2000)
        
        # 2. Traitement des features
        processed_df, feature_info = demo_feature_processing(spark, demo_data)
        
        # 3. Entra√Ænement des mod√®les
        models_results, test_df = demo_model_training(spark, processed_df, feature_info)
        
        # 4. √âvaluation des mod√®les
        evaluation_results = demo_model_evaluation(spark, models_results, test_df, feature_info)
        
        # 5. Service de pr√©diction
        demo_prediction_service(spark, models_results, feature_info)
        
        # R√©sum√© final
        total_time = time.time() - start_time
        print("\n" + "="*80)
        print("üéâ D√âMONSTRATION TERMIN√âE AVEC SUCC√àS!")
        print("="*80)
        print(f"‚è±Ô∏è  Temps total: {total_time:.2f} secondes")
        print(f"üìä Donn√©es trait√©es: {demo_data.count()} accidents")
        print(f"ü§ñ Mod√®les entra√Æn√©s: {len(models_results)}")
        print(f"üìà √âvaluations effectu√©es: {len(evaluation_results)}")
        print(f"üîß Features utilis√©es: {len(feature_info['selected_features'])}")
        
        print(f"\nüìã Composants test√©s:")
        print(f"   ‚úÖ FeatureProcessor - Pr√©paration des features")
        print(f"   ‚úÖ ModelTrainer - Entra√Ænement des mod√®les")
        print(f"   ‚úÖ ModelEvaluator - √âvaluation compl√®te")
        print(f"   ‚úÖ PredictionService - Service de pr√©diction")
        
        print(f"\nüèóÔ∏è  Architecture Medallion:")
        print(f"   Bronze (FEEDER) ‚Üí Silver (PREPROCESSOR) ‚Üí Gold (DATAMART) ‚Üí ML (MLTRAINING)")
        print(f"   Donn√©es brutes ‚Üí Features enrichies ‚Üí Analytics ‚Üí Mod√®les ML")
        
        print(f"\nüéØ L'application MLTRAINING est pr√™te pour la production!")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la d√©monstration: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        spark.stop()
        print("\nüîö Session Spark ferm√©e")


if __name__ == "__main__":
    main()