#!/usr/bin/env python3
"""
Script de dÃ©monstration pour l'application MLTRAINING

Ce script teste l'application MLTRAINING complÃ¨te avec des donnÃ©es simulÃ©es
pour dÃ©montrer toutes les fonctionnalitÃ©s de classification de sÃ©vÃ©ritÃ©.
"""

import os
import sys
import time
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
import pandas as pd

# Ajout du chemin pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.common.config.config_manager import ConfigManager
from src.common.utils.logger import Logger
from src.applications.mltraining.mltraining_app import MLTrainingApp
from src.applications.mltraining.feature_processor import FeatureProcessor
from src.applications.mltraining.model_trainer import ModelTrainer
from src.applications.mltraining.model_evaluator import ModelEvaluator
from src.applications.mltraining.prediction_service import PredictionService


def create_demo_data(spark: SparkSession, num_records: int = 1000):
    """CrÃ©e des donnÃ©es de dÃ©monstration pour l'entraÃ®nement ML"""
    print(f"ğŸ”§ CrÃ©ation de {num_records} enregistrements de dÃ©monstration...")
    
    # SchÃ©ma des donnÃ©es
    schema = StructType([
        StructField("Severity", IntegerType(), True),
        StructField("Distance_mi", DoubleType(), True),
        StructField("Temperature_F", DoubleType(), True),
        StructField("Wind_Chill_F", DoubleType(), True),
        StructField("Humidity_percent", DoubleType(), True),
        StructField("Pressure_in", DoubleType(), True),
        StructField("Visibility_mi", DoubleType(), True),
        StructField("Wind_Speed_mph", DoubleType(), True),
        StructField("Precipitation_in", DoubleType(), True),
        StructField("Start_Lat", DoubleType(), True),
        StructField("Start_Lng", DoubleType(), True),
        StructField("State", StringType(), True),
        StructField("Weather_Condition", StringType(), True),
        StructField("Source", StringType(), True),
        StructField("Sunrise_Sunset", StringType(), True),
        StructField("Civil_Twilight", StringType(), True),
        StructField("Nautical_Twilight", StringType(), True),
        StructField("Astronomical_Twilight", StringType(), True),
        StructField("Amenity", StringType(), True),
        StructField("Bump", StringType(), True),
        StructField("Crossing", StringType(), True),
        StructField("Give_Way", StringType(), True),
        StructField("Junction", StringType(), True),
        StructField("No_Exit", StringType(), True),
        StructField("Railway", StringType(), True),
        StructField("Roundabout", StringType(), True),
        StructField("Station", StringType(), True),
        StructField("Stop", StringType(), True),
        StructField("Traffic_Calming", StringType(), True),
        StructField("Traffic_Signal", StringType(), True),
        StructField("Turning_Loop", StringType(), True),
        StructField("accident_hour", IntegerType(), True),
        StructField("accident_day_of_week", IntegerType(), True),
        StructField("accident_month", IntegerType(), True),
        StructField("accident_season", StringType(), True),
        StructField("is_weekend", StringType(), True),
        StructField("is_rush_hour", StringType(), True),
        StructField("weather_category", StringType(), True),
        StructField("weather_severity_score", DoubleType(), True),
        StructField("visibility_category", StringType(), True),
        StructField("temperature_category", StringType(), True),
        StructField("distance_to_city_center", DoubleType(), True),
        StructField("urban_rural_classification", StringType(), True),
        StructField("state_region", StringType(), True),
        StructField("population_density_category", StringType(), True),
        StructField("infrastructure_count", IntegerType(), True),
        StructField("safety_equipment_score", DoubleType(), True),
        StructField("traffic_control_type", StringType(), True)
    ])
    
    # GÃ©nÃ©ration de donnÃ©es rÃ©alistes
    import random
    random.seed(42)
    
    data = []
    states = ["CA", "NY", "TX", "FL", "IL", "PA", "OH", "GA", "NC", "MI"]
    weather_conditions = ["Clear", "Rain", "Snow", "Cloudy", "Fog", "Windy"]
    sources = ["Source1", "Source2", "Source3"]
    regions = ["West", "Northeast", "South", "Midwest"]
    
    for i in range(num_records):
        # SÃ©vÃ©ritÃ© avec distribution rÃ©aliste (plus de cas mineurs)
        severity_weights = [0.4, 0.35, 0.2, 0.05]  # 1, 2, 3, 4
        severity = random.choices([1, 2, 3, 4], weights=severity_weights)[0]
        
        # Variables corrÃ©lÃ©es avec la sÃ©vÃ©ritÃ©
        base_distance = 0.1 + (severity - 1) * 0.5 + random.uniform(-0.2, 0.2)
        base_visibility = 15.0 - (severity - 1) * 3.0 + random.uniform(-2, 2)
        base_weather_score = severity * 0.5 + random.uniform(0, 1)
        
        # CoordonnÃ©es gÃ©ographiques rÃ©alistes (Ã‰tats-Unis)
        lat = 25.0 + random.uniform(0, 25)  # 25-50Â°N
        lng = -125.0 + random.uniform(0, 50)  # -125 Ã  -75Â°W
        
        record = (
            severity,
            max(0.1, base_distance),  # Distance
            random.uniform(20, 100),  # Temperature
            random.uniform(15, 95),   # Wind Chill
            random.uniform(20, 100),  # Humidity
            random.uniform(28, 32),   # Pressure
            max(0.1, base_visibility), # Visibility
            random.uniform(0, 30),    # Wind Speed
            random.uniform(0, 2),     # Precipitation
            lat, lng,                 # Coordinates
            random.choice(states),    # State
            random.choice(weather_conditions),  # Weather
            random.choice(sources),   # Source
            random.choice(["Day", "Night"]),    # Sunrise_Sunset
            random.choice(["Day", "Night"]),    # Civil_Twilight
            random.choice(["Day", "Night"]),    # Nautical_Twilight
            random.choice(["Day", "Night"]),    # Astronomical_Twilight
            # Infrastructure boolÃ©enne (13 colonnes)
            *[random.choice(["True", "False"]) for _ in range(13)],
            # Features temporelles
            random.randint(0, 23),    # hour
            random.randint(1, 7),     # day_of_week
            random.randint(1, 12),    # month
            random.choice(["Spring", "Summer", "Fall", "Winter"]),  # season
            random.choice(["True", "False"]),  # is_weekend
            random.choice(["True", "False"]),  # is_rush_hour
            # Features mÃ©tÃ©orologiques
            random.choice(["Clear", "Adverse", "Moderate"]),  # weather_category
            base_weather_score,       # weather_severity_score
            random.choice(["Good", "Poor", "Moderate"]),      # visibility_category
            random.choice(["Cold", "Mild", "Hot"]),           # temperature_category
            # Features gÃ©ographiques
            random.uniform(0, 50),    # distance_to_city_center
            random.choice(["Urban", "Rural", "Suburban"]),    # urban_rural
            random.choice(regions),   # state_region
            random.choice(["Low", "Medium", "High"]),         # population_density
            # Features infrastructure
            random.randint(0, 5),     # infrastructure_count
            random.uniform(1, 5),     # safety_equipment_score
            random.choice(["Signal", "Stop", "None"])         # traffic_control_type
        )
        
        data.append(record)
    
    df = spark.createDataFrame(data, schema)
    print(f"âœ… DonnÃ©es crÃ©Ã©es: {df.count()} enregistrements, {len(df.columns)} colonnes")
    
    # Affichage de la distribution des sÃ©vÃ©ritÃ©s
    severity_dist = df.groupBy("Severity").count().orderBy("Severity").collect()
    print("ğŸ“Š Distribution des sÃ©vÃ©ritÃ©s:")
    for row in severity_dist:
        print(f"   SÃ©vÃ©ritÃ© {row['Severity']}: {row['count']} accidents")
    
    return df


def demo_feature_processing(spark: SparkSession, data: any):
    """DÃ©monstration du traitement des features"""
    print("\n" + "="*60)
    print("ğŸ”§ DÃ‰MONSTRATION - TRAITEMENT DES FEATURES")
    print("="*60)
    
    config = ConfigManager()
    feature_processor = FeatureProcessor(config, spark)
    
    print("ğŸ“‹ Analyse initiale des donnÃ©es...")
    start_time = time.time()
    
    # PrÃ©paration des features
    processed_df, feature_info = feature_processor.prepare_features_for_ml(data)
    
    processing_time = time.time() - start_time
    
    print(f"âœ… Traitement terminÃ© en {processing_time:.2f} secondes")
    print(f"ğŸ“Š Features sÃ©lectionnÃ©es: {len(feature_info['selected_features'])}")
    print(f"ğŸ¯ Colonne cible: {feature_info['target_column']}")
    print(f"ğŸ“ˆ MÃ©triques de traitement:")
    
    for metric, value in feature_info['processing_metrics'].items():
        print(f"   {metric}: {value}")
    
    return processed_df, feature_info


def demo_model_training(spark: SparkSession, processed_df: any, feature_info: dict):
    """DÃ©monstration de l'entraÃ®nement des modÃ¨les"""
    print("\n" + "="*60)
    print("ğŸ¤– DÃ‰MONSTRATION - ENTRAÃNEMENT DES MODÃˆLES")
    print("="*60)
    
    config = ConfigManager()
    trainer = ModelTrainer(config, spark)
    
    # Division des donnÃ©es
    train_df, temp_df = processed_df.randomSplit([0.8, 0.2], seed=42)
    val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)
    
    print(f"ğŸ“Š Division des donnÃ©es:")
    print(f"   EntraÃ®nement: {train_df.count()} Ã©chantillons")
    print(f"   Validation: {val_df.count()} Ã©chantillons")
    print(f"   Test: {test_df.count()} Ã©chantillons")
    
    print("\nğŸš€ EntraÃ®nement des modÃ¨les...")
    start_time = time.time()
    
    # EntraÃ®nement de modÃ¨les individuels pour la dÃ©mo
    models_results = {}
    
    # Random Forest
    print("   ğŸŒ² EntraÃ®nement Random Forest...")
    rf_result = trainer.train_single_model(
        'random_forest', train_df, val_df, 
        feature_info['selected_features'], 'Severity'
    )
    models_results['random_forest'] = rf_result
    
    # Logistic Regression (plus rapide pour la dÃ©mo)
    print("   ğŸ“ˆ EntraÃ®nement Logistic Regression...")
    lr_result = trainer.train_single_model(
        'logistic_regression', train_df, val_df,
        feature_info['selected_features'], 'Severity'
    )
    models_results['logistic_regression'] = lr_result
    
    training_time = time.time() - start_time
    
    print(f"\nâœ… EntraÃ®nement terminÃ© en {training_time:.2f} secondes")
    print("ğŸ“Š RÃ©sultats des modÃ¨les:")
    
    for model_name, result in models_results.items():
        metrics = result['validation_metrics']
        print(f"\n   {model_name.upper()}:")
        print(f"     Accuracy: {metrics['accuracy']:.4f}")
        print(f"     F1-Score: {metrics['f1_score_macro']:.4f}")
        print(f"     Precision: {metrics['precision_macro']:.4f}")
        print(f"     Recall: {metrics['recall_macro']:.4f}")
        print(f"     Temps d'entraÃ®nement: {result['training_time_seconds']:.2f}s")
    
    return models_results, test_df


def demo_model_evaluation(spark: SparkSession, models_results: dict, test_df: any, feature_info: dict):
    """DÃ©monstration de l'Ã©valuation des modÃ¨les"""
    print("\n" + "="*60)
    print("ğŸ“Š DÃ‰MONSTRATION - Ã‰VALUATION DES MODÃˆLES")
    print("="*60)
    
    config = ConfigManager()
    evaluator = ModelEvaluator(config, spark)
    
    evaluation_results = []
    
    for model_name, model_result in models_results.items():
        print(f"\nğŸ” Ã‰valuation du modÃ¨le {model_name.upper()}...")
        
        evaluation = evaluator.evaluate_model(
            model=model_result['model'],
            test_df=test_df,
            feature_columns=feature_info['selected_features'],
            target_column='Severity',
            model_name=model_name
        )
        
        evaluation_results.append(evaluation)
        
        print(f"ğŸ“ˆ MÃ©triques globales:")
        global_metrics = evaluation['global_metrics']
        for metric, value in global_metrics.items():
            print(f"   {metric}: {value:.4f}")
        
        print(f"ğŸ“‹ MÃ©triques par classe:")
        class_metrics = evaluation['class_metrics']
        for class_name, metrics in class_metrics.items():
            print(f"   {class_name}: F1={metrics['f1_score']:.3f}, "
                  f"Precision={metrics['precision']:.3f}, "
                  f"Recall={metrics['recall']:.3f}, "
                  f"Support={metrics['support']}")
        
        print(f"ğŸ¯ Matrice de confusion:")
        confusion = evaluation['confusion_matrix']
        print(f"   Total Ã©chantillons: {confusion['total_samples']}")
        
        if evaluation['feature_importance']:
            print(f"ğŸ” Top 5 features importantes:")
            top_features = list(evaluation['feature_importance'].items())[:5]
            for feature, importance in top_features:
                print(f"   {feature}: {importance:.4f}")
    
    # Comparaison des modÃ¨les
    if len(evaluation_results) > 1:
        print(f"\nğŸ† COMPARAISON DES MODÃˆLES:")
        comparison = evaluator.compare_models(evaluation_results)
        
        if comparison.get('ranking_by_f1_macro'):
            print("ğŸ“Š Classement par F1-Score:")
            for i, model_data in enumerate(comparison['ranking_by_f1_macro'], 1):
                print(f"   {i}. {model_data['model_name']}: {model_data['f1_score_macro']:.4f}")
        
        best_model = comparison.get('overall_best_model')
        if best_model:
            print(f"ğŸ¥‡ Meilleur modÃ¨le: {best_model}")
    
    return evaluation_results


def demo_prediction_service(spark: SparkSession, models_results: dict, feature_info: dict):
    """DÃ©monstration du service de prÃ©diction"""
    print("\n" + "="*60)
    print("ğŸ”® DÃ‰MONSTRATION - SERVICE DE PRÃ‰DICTION")
    print("="*60)
    
    config = ConfigManager()
    predictor = PredictionService(config, spark)
    
    # Simulation du chargement d'un modÃ¨le (on utilise le premier modÃ¨le disponible)
    if models_results:
        model_name = list(models_results.keys())[0]
        model = models_results[model_name]['model']
        
        print(f"ğŸ“¥ Simulation du chargement du modÃ¨le: {model_name}")
        
        # Simulation d'une prÃ©diction unitaire
        print("\nğŸ¯ PrÃ©diction unitaire:")
        sample_accident = {
            'Distance_mi': 0.8,
            'Temperature_F': 45.0,
            'Humidity_percent': 85.0,
            'Pressure_in': 29.5,
            'Visibility_mi': 3.0,
            'Start_Lat': 40.7128,
            'Start_Lng': -74.0060
        }
        
        print("ğŸ“‹ DonnÃ©es d'entrÃ©e:")
        for key, value in sample_accident.items():
            print(f"   {key}: {value}")
        
        # Conversion en DataFrame pour la prÃ©diction
        input_df = predictor._dict_to_dataframe(sample_accident)
        
        # Preprocessing et prÃ©diction simulÃ©e
        try:
            processed_input = predictor._preprocess_for_prediction(input_df)
            predictions = model.transform(processed_input)
            
            # Extraction du rÃ©sultat
            if predictions.count() > 0:
                result_row = predictions.collect()[0]
                predicted_severity = int(result_row['prediction'])
                severity_names = {1: 'Minor', 2: 'Moderate', 3: 'Serious', 4: 'Severe'}
                
                print(f"\nâœ… RÃ©sultat de la prÃ©diction:")
                print(f"   SÃ©vÃ©ritÃ© prÃ©dite: {predicted_severity} ({severity_names[predicted_severity]})")
                print(f"   Confiance: Ã‰levÃ©e")
            else:
                print("âŒ Erreur lors de la prÃ©diction")
                
        except Exception as e:
            print(f"âš ï¸  Simulation de prÃ©diction (erreur attendue): {str(e)[:100]}...")
            print("   (Normal en mode dÃ©mo sans modÃ¨le MLflow complet)")
    
    # Informations sur le service
    print(f"\nğŸ“Š Informations du service:")
    model_info = predictor.get_model_info()
    for key, value in model_info.items():
        if key != 'feature_columns':  # Ã‰viter l'affichage de la longue liste
            print(f"   {key}: {value}")
    
    metrics = predictor.get_service_metrics()
    print(f"ğŸ“ˆ MÃ©triques du service:")
    for key, value in metrics.items():
        print(f"   {key}: {value}")


def main():
    """Fonction principale de dÃ©monstration"""
    print("ğŸš€ DÃ‰MONSTRATION COMPLÃˆTE - APPLICATION MLTRAINING")
    print("=" * 80)
    print("Classification de sÃ©vÃ©ritÃ© des accidents de la route")
    print("Architecture Medallion - Couche ML")
    print("=" * 80)
    
    # Configuration Spark pour la dÃ©mo
    spark = SparkSession.builder \
        .appName("MLTraining-Demo") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")  # RÃ©duire les logs Spark
    
    try:
        start_time = time.time()
        
        # 1. CrÃ©ation des donnÃ©es de dÃ©monstration
        demo_data = create_demo_data(spark, num_records=2000)
        
        # 2. Traitement des features
        processed_df, feature_info = demo_feature_processing(spark, demo_data)
        
        # 3. EntraÃ®nement des modÃ¨les
        models_results, test_df = demo_model_training(spark, processed_df, feature_info)
        
        # 4. Ã‰valuation des modÃ¨les
        evaluation_results = demo_model_evaluation(spark, models_results, test_df, feature_info)
        
        # 5. Service de prÃ©diction
        demo_prediction_service(spark, models_results, feature_info)
        
        # RÃ©sumÃ© final
        total_time = time.time() - start_time
        print("\n" + "="*80)
        print("ğŸ‰ DÃ‰MONSTRATION TERMINÃ‰E AVEC SUCCÃˆS!")
        print("="*80)
        print(f"â±ï¸  Temps total: {total_time:.2f} secondes")
        print(f"ğŸ“Š DonnÃ©es traitÃ©es: {demo_data.count()} accidents")
        print(f"ğŸ¤– ModÃ¨les entraÃ®nÃ©s: {len(models_results)}")
        print(f"ğŸ“ˆ Ã‰valuations effectuÃ©es: {len(evaluation_results)}")
        print(f"ğŸ”§ Features utilisÃ©es: {len(feature_info['selected_features'])}")
        
        print(f"\nğŸ“‹ Composants testÃ©s:")
        print(f"   âœ… FeatureProcessor - PrÃ©paration des features")
        print(f"   âœ… ModelTrainer - EntraÃ®nement des modÃ¨les")
        print(f"   âœ… ModelEvaluator - Ã‰valuation complÃ¨te")
        print(f"   âœ… PredictionService - Service de prÃ©diction")
        
        print(f"\nğŸ—ï¸  Architecture Medallion:")
        print(f"   Bronze (FEEDER) â†’ Silver (PREPROCESSOR) â†’ Gold (DATAMART) â†’ ML (MLTRAINING)")
        print(f"   DonnÃ©es brutes â†’ Features enrichies â†’ Analytics â†’ ModÃ¨les ML")
        
        print(f"\nğŸ¯ L'application MLTRAINING est prÃªte pour la production!")
        
    except Exception as e:
        print(f"\nâŒ Erreur lors de la dÃ©monstration: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        spark.stop()
        print("\nğŸ”š Session Spark fermÃ©e")


if __name__ == "__main__":
    main()